"use strict";(self.webpackChunkdsd_project=self.webpackChunkdsd_project||[]).push([[5053],{711:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>m,frontMatter:()=>s,metadata:()=>d,toc:()=>l});var r=t(4848),i=t(8453);const s={sidebar_label:"C\xe1c lo\u1ea1i Pooling"},a="C\xe1c lo\u1ea1i Pooling",d={id:"pytorch/Deberta/Pooling",title:"C\xe1c lo\u1ea1i Pooling",description:"",source:"@site/docs/pytorch/00-Deberta/Pooling.md",sourceDirName:"pytorch/00-Deberta",slug:"/pytorch/Deberta/Pooling",permalink:"/docs/pytorch/Deberta/Pooling",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{sidebar_label:"C\xe1c lo\u1ea1i Pooling"},sidebar:"tailieuSidebar",previous:{title:"Ph\xe2n lo\u1ea1i d\u1eef li\u1ec7u text v\u1edbi Deberta Sequence Classification",permalink:"/docs/pytorch/Deberta/DebertaForSequenceClassification"},next:{title:"H\u01b0\u1edbng d\u1eabn c\xe0i \u0111\u1eb7t Postgres",permalink:"/docs/postgres/"}},o={},l=[];function _(e){const n={code:"code",h1:"h1",pre:"pre",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"c\xe1c-lo\u1ea1i-pooling",children:"C\xe1c lo\u1ea1i Pooling"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"class MeanPooling(nn.Module):\r\n    def __init__(self):\r\n        super(MeanPooling, self).__init__()\r\n        \r\n    def forward(self, last_hidden_state, attention_mask):\r\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\r\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\r\n        sum_mask = input_mask_expanded.sum(1)\r\n        sum_mask = torch.clamp(sum_mask, min = 1e-9)\r\n        mean_embeddings = sum_embeddings/sum_mask\r\n        return mean_embeddings\r\n\r\nclass MaxPooling(nn.Module):\r\n    def __init__(self):\r\n        super(MaxPooling, self).__init__()\r\n        \r\n    def forward(self, last_hidden_state, attention_mask):\r\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\r\n        embeddings = last_hidden_state.clone()\r\n        embeddings[input_mask_expanded == 0] = -1e4\r\n        max_embeddings, _ = torch.max(embeddings, dim = 1)\r\n        return max_embeddings\r\n    \r\nclass MinPooling(nn.Module):\r\n    def __init__(self):\r\n        super(MinPooling, self).__init__()\r\n        \r\n    def forward(self, last_hidden_state, attention_mask):\r\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\r\n        embeddings = last_hidden_state.clone()\r\n        embeddings[input_mask_expanded == 0] = 1e-4\r\n        min_embeddings, _ = torch.min(embeddings, dim = 1)\r\n        return min_embeddings\r\n\r\n#Attention pooling\r\nclass AttentionPooling(nn.Module):\r\n    def __init__(self, in_dim):\r\n        super().__init__()\r\n        self.attention = nn.Sequential(\r\n        nn.Linear(in_dim, in_dim),\r\n        nn.LayerNorm(in_dim),\r\n        nn.GELU(),\r\n        nn.Linear(in_dim, 1),\r\n        )\r\n\r\n    def forward(self, last_hidden_state, attention_mask):\r\n        w = self.attention(last_hidden_state).float()\r\n        w[attention_mask==0]=float('-inf')\r\n        w = torch.softmax(w,1)\r\n        attention_embeddings = torch.sum(w * last_hidden_state, dim=1)\r\n        return attention_embeddings\r\n\r\n#There may be a bug in my implementation because it does not work well.\r\nclass WeightedLayerPooling(nn.Module):\r\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\r\n        super(WeightedLayerPooling, self).__init__()\r\n        self.layer_start = layer_start\r\n        self.num_hidden_layers = num_hidden_layers\r\n        self.layer_weights = layer_weights if layer_weights is not None \\\r\n            else nn.Parameter(\r\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\r\n            )\r\n\r\n    def forward(self, ft_all_layers):\r\n        all_layer_embedding = torch.stack(ft_all_layers)\r\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\r\n\r\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\r\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\r\n\r\n        return weighted_average\n"})})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(_,{...e})}):_(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>d});var r=t(6540);const i={},s=r.createContext(i);function a(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);